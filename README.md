# VRM Talk 統合環境

キャラクター（VRM）とリアルタイムで会話するための、shisaku・COEIROINK・Ollama を連携させたフルスタック環境です。
COEIROINKのDocker環境は、以下をベースに構築していますが、安定動作のために改造を加えています。また、ライセンスが不明なため、これは個人の開発用スタックとして構成されており、再現環境の構築を目的としていません。
[kuwacom/COEIROINK-with-Docker](https://github.com/kuwacom/COEIROINK-with-Docker)

また、shisaku アプリケーションの構築にあたり、以下の記事を参考にしています。
[three-vrmを使ってブラウザ上にVRMを表示してみた](https://zenn.dev/yushimaten/articles/a853e3328db695)
## ✨ 機能紹介

- **デュアルAIエンジン**: 高性能なクラウドAI (Google Gemini) と、プライベートなローカルAI (Ollama) を、会話の目的に応じて切り替えられます。
- **感情連動型の音声合成**: AIが会話の文脈や感情を読み取り、COEIROINKの多彩な音声スタイル（喜び、しょんぼり、ごきげん等）を自動で選択します。表現力豊かな声でキャラクターが話します。
- **ダイナミックなアバター動作**: 会話内容に応じてキャラクターの首の動きなどを自動生成し、生き生きとしたインタラクションを実現します。
- **音声入力対応**: マイクに向かって話しかけることで、文字を打ずに会話を進められます。
- **Dockerによる簡単構築**: Docker Compose を使うことで、複雑な環境をコマンド一つで再現できます。

> [!NOTE]
> 実際の動作の様子をデモ動画として公開しています。AIとの会話に応じてキャラクターの音声や表情、動作がリアルタイムで連携する様子をご覧いただけます。セキュリティの観点から、サーバーの一般公開は見合わせています。
>
> 動画で使用したモデル
> [フロウ](https://hub.vroid.com/characters/471902356051310196/models/1057268305479785163)
> [オルテンシア](https://hub.vroid.com/characters/2275990777783334731/models/5546300287879782053)

---

**注意:** 以下の「前提条件」から「トラブルシューティング」までの内容は、この環境が再現環境の構築を目的としていないため、記載通りに実行しても動作しません。あくまで参考情報としてご覧ください。

<del>

## 🛠️ 前提条件

この環境を動作させるには、お使いのPCに以下のソフトウェアがインストールされている必要があります。

- **Docker と Docker Compose**
- **NVIDIA製GPU** と、対応するグラフィックドライバ
  - > ローカルLLMを使用する場合、NVIDIA GeForce RTX 4060と同等かそれ以上を推奨します。
- **NVIDIA Container Toolkit**: DockerコンテナからGPUを利用するために必須です。
- **Git**: このリポジトリをクローンするために使用します。

> [!IMPORTANT]
> **(WSL2ユーザー向け)** WSL2のメモリ割り当てが少ないと、Ollamaの動作が極端に遅くなる場合があります。
> `%userprofile%` フォルダに `.wslconfig` ファイルを作成し、十分なメモリを割り当てることを推奨します。
> ```ini
> [wsl2]
> memory=16GB
> ```
> 設定後は `wsl --shutdown` コマンドでWSL2を再起動してください。

---

## 🚀 環境構築の手順

### ステップ1: リポジトリをクローン
```bash
git clone https://github.com/Qoo2298/vrm_talk.git
cd vrm_talk
```

### ステップ2: 環境変数の設定
`.env` という名前のファイルを、この `README.md` と同じ階層に作成し、中にご自身のGemini APIキーを記述してください。

```plaintext:C:/Users/shiny/AppData/Local/Programs/Microsoft VS Code/Ubuntu/home/owner/stack/.env
GEMINI_API_KEY=ここにあなたのAPIキーを貼り付け
```

### ステップ3: 不足しているアセットの配置 (重要！)
このリポジトリには、ライセンスや容量の都合上、モデルやエンジンそのものは含まれていません。以下の手順で、必要なアセットをダウンロードし、正しい場所に配置してください。

#### ① COEIROINK (エンジン & 音声モデル)

1.  **COEIROINK公式サイト**から、Linux GPU版の本体（.zip）をダウンロードします。
    - https://coeiroink.com/download
2.  ダウンロードしたzipファイルを展開し、`COEIROINK_LINUX_GPU` フォルダを、このリポジトリの `COEIROINK-with-Docker` フォルダの直下に配置します。
3.  **【重要】** まず、配置したフォルダ（`COEIROINK-with-Docker/COEIROINK_LINUX_GPU`）の中にあるGUIアプリケーション本体（`COEIROINK`など）を一度起動してください。
4.  アプリが起動したら、ソフトの画面上で使用したい音声モデルをすべてダウンロードします。
    - ダウンロードされたモデルは、自動的に、先ほど配置した `COEIROINK-with-Docker/COEIROINK_LINUX_GPU` フォルダ内の `speaker_info` フォルダに保存されていきます。
5.  必要なモデルをすべてダウンロードしたら、`COEIROINK_LINUX_GPU` フォルダ内にある `engine` フォルダと、モデルが保存された `speaker_info` フォルダの両方を、このリポジトリ（`vrm_talk`）の `COEIROINK-with-Docker` フォルダの直下にコピー（または移動）します。
    - （※つまり、`engine` と `speaker_info` が、`COEIROINK_LINUX_GPU` フォルダと同じ階層に来るようにします）

> [!TIP]
> 現状の `shisaku` アプリケーションでは、音声モデルとしてデフォルトで「MANA」が使用されます。
> 他の音声モデルを使用したい場合は、そのモデルの `speaker_id` と `style_id` を特定し、`shisaku` 側の設定ファイルを修正する必要があります。
> 詳細な設定方法やファイルの場所については、AIアシスタントに尋ねてみてください。

#### ② VRMモデル

1.  お好きなVRMモデルを用意します。ここでは例として、以下のモデルを使用します。
    - **VRoid Hub**: 「オルテンシア」
2.  ダウンロードしたVRMファイルを `avatar.vrm` という名前に変更します。
3.  名前を変更した `avatar.vrm` ファイルを、`shisaku/frontend/` フォルダの中に配置します。

> [!NOTE]
> VRMモデルを変更する場合、`shisaku`側の設定でデフォルト（例: `default.vrm`）となっている箇所を、配置したファイル名（例: `avatar.vrm`）に修正する必要がある場合があります。

#### ③ モーションデータ

1.  `shisaku` が使用するモーションデータを、以下のBOOTHページからダウンロードします。
    - **BOOTH**: VRMおしゃべりモーション
2.  `shisaku/frontend/` フォルダの中に、新しく `vrma` という名前のフォルダを作成してください。（このフォルダはデフォルトでは存在しません）
3.  ダウンロードしたフォルダの中身を、すべて新しく作成した `vrma` フォルダの中に配置します。

### 最終的なプロジェクトのディレクトリ構成
すべてのファイルとアセットを配置すると、プロジェクトのトップレベルは以下のようになります。

```plaintext
vrm_talk/
├── .env                    # 環境変数ファイル
├── .gitignore
├── COEIROINK-with-Docker/  # COEIROINKエンジンとモデル
│   ├── engine/             # 👈 配置したエンジン
│   ├── speaker_info/       # 👈 配置した音声モデル
│   └── ...
├── ollama/                 # Ollamaのデータボリューム
├── shisaku/                # shisakuアプリケーション本体
│   ├── backend/
│   └── frontend/
│       ├── avatar.vrm      # 👈 配置したVRMモデル
│       └── vrma/           # 👈 配置したモーションデータ
│           └── ...
├── docker-compose.yml
└── README.md
```

### ステップ4: コンテナのビルドと起動
すべてのアセットを配置したら、以下のコマンドで3つのコンテナをまとめてビルDし、バックグラウンドで起動します。

```bash
docker compose up -d --build
```

### ステップ5: Ollama AIモデルのダウンロード
コンテナは起動しましたが、まだOllamaの中にAIモデルが入っていません。
以下のコマンドを実行して、shisaku が使用する2つのモデルをダウンロードしてください。（完了まで少し時間がかかります）

```bash
# 軽量モデル
docker compose exec ollama ollama pull gemma:2b

# 高性能モデル (任意)
docker compose exec ollama ollama pull llama3
```

---

## 🖥️ アプリケーションの使用方法
全てのセットアップが完了したら、以下のURLにブラウザでアクセスしてください。

- **メイン画面**: `http://localhost:8000`

各APIサーバーの動作確認は、以下のURLから行えます。
- **COEIROINK API**: `http://localhost:50032`
- **Ollama API**: `http://localhost:11434`

### ネットワークアクセスについて (重要！)
デフォルトの docker-compose.yml の設定 (ports: - "8000:8000") では、アプリケーションはそのPC自身 (localhost) からのアクセスのみを受け付けます。

<details>
<summary><strong>同じLAN内の他のデバイス (スマホ、別のPCなど) からアクセスしたい場合</strong></summary>

`docker-compose.yml` 内の各サービスの `ports` 設定を、以下のように変更してください。

```yaml
# 例: shisakuサービスの場合
services:
  shisaku:
    ports:
      - "0.0.0.0:8000:8000"
```

`0.0.0.0` を追加することで、そのPCのすべてのネットワークインターフェースからのアクセスを受け付けるようになります。変更後は `docker compose up -d --force-recreate` でコンテナを再作成してください。

</details>

<details>
<summary><strong>インターネット (グローバル) からアクセスしたい場合</strong></summary>

`0.0.0.0` の設定に加えて、以下の設定が必要です。

- **ルーターのポートフォワーディング**: インターネットの出入り口にあるルーターで、外部からのアクセスをサーバーの該当ポートに転送する設定が必要です。
- **サーバーのファイアウォール設定**: サーバー自身のファイアウォールで、該当ポートへのアクセスを許可する設定が必要です。

> [!WARNING]
> これらの設定を行わない限り、`0.0.0.0` に設定してもインターネットから直接公開されることはありません。セキュリティ設定は慎重に行ってください。

</details>

---

## 🤔 トラブルシューティング

- **コンテナが起動しない**:
  `docker compose up` を `-d` なしで実行すると、詳細なログを確認できます。ポートの競合（`address already in use`）が起きている場合は、他のアプリが同じポートを使っていないか確認するか、`docker-compose.yml` の `ports` の設定を変更してください。

- **Ollamaの応答が異常に遅い**:
  WSL2のメモリ不足が考えられます。「前提条件」の `.wslconfig` の設定を確認してください。

</del>

---

## 📝 開発後記
この環境の構築は、私にとって非常に挑戦的で、多くの学びがありました。特に印象的だったのは、以下の点です。

### 苦労した点
- **COEIROINKとの格闘**: Linux版は提供されているものの、GUIを前提とした設計のため、CLI環境で安定動作させるまでに大変な苦労がありました。参照サイトを読み込み、何度も試行錯誤を繰り返しましたが、エラーの原因が分からず、途方に暮れることも少なくありませんでした。
- **DockerとGitとの出会い**: 開発当初はDockerやGitの知識が全くなく、これらのツールを理解し、連携させることに多くの時間を費やしました。特にGitHubへの初めてのプッシュは、コミットの概念やアクセストークンでの認証など、慣れない作業の連続で悪戦苦闘しました。
- **WindowsからLinuxへの移行**: 最初はWindows環境で動作させていましたが、サーバー運用を視野に入れLinux環境への移行を決断しました。しかし、WindowsのGUIで動いていたCOEIROINKが、LinuxのCLI環境ではそのままでは動作しないという壁に直面し、この点が最も困難な課題の一つでした。

### 開発のパートナー、生成AI
- **Geminiとの協業**: 開発のほとんどの段階で、Geminiの助けを借りました。特に、エラーの原因究明やコードの記述において、そのサポートは不可欠でした。多くのコードはGeminiに書いてもらい、私自身は試行錯誤と検証に集中することができました。

このプロジェクトを通じて、Dockerによる環境構築の重要性、Gitによるバージョン管理の便利さ、そしてAIとの協業の可能性を深く実感しました。長い道のりでしたが、最終的にこの環境が動作したときの感動は忘れられません。このリポジトリが、同じような課題に直面する方々の一助となれば幸いです。

### 今後の課題と考察
表情や動作をLLMに委ねると、どうしてもラグが発生してしまうため、両立が大事だと感じました。
12Bクラスのモデルも試しましたが、ローカル環境では遅延を考慮すると軽量なモデルが妥当という結論に至りました。
現状ではクラウドのGemini APIを使用した場合でも若干の遅延は発生してしまうため、今後はバックグラウンド処理を工夫するなどして、この遅延をさらに短縮できるようにしたいと考えています。

ただ、知識がない状態で生成AIにすべてを委ねるのは大変で、頼りすぎるのもよくないと感じます。
> やはり現場ではセキュリティ等の問題で生成AIが使えないこともあるため、自身である程度はコードが書けるようになる必要性を実感しました。
> 生成AIだと、どうしてもWeb UIがズレてしまうことがあります。いずれは完璧になると思われますが、現状では微調整が必要です。
> エラーがなぜ発生し、なぜ解決したかがわからない場面も多々ありました。
> 実際に実装は大変苦戦し、現場の方々の気持ちが少しわかったような気がします。

### 開発の動機と未来への展望
そもそも、このような統合環境を開発することには以前から憧れがありました。コードを書く技術は持ち合わせていませんでしたが、IT業界を志望する中で、最近では生成AIがコード作成や相談相手になってくれることを知り、このプロジェクトに着手しました。

もともとはLLMと動作データ、視聴覚データ、表情を融合させた統合モデルを再現したいと考えていましたが、現状ではどうしても統合が難しく、ぎこちない動作になってしまいます。これは、そのうちビッグテックが実現してくれると信じています。このプロジェクトは、その壮大な未来への私Nの第一歩です。

X（旧Twitter）では、Grokを活用したAniのような事例がありますが、このプロジェクトはその延長線上にある未来を意識しています。キャラクターがただ話すだけでなく、感情や文脈に合わせて自然に動き、より豊かなコミュニケーションが生まれる世界を目指しています。

現在、ヒューマノイドロボットにも強い関心があり、人間とAIがより密接に共生する未来の実現に貢献したいと考えています。

### GitHub初心者からのメッセージ
GitHubの利用にはまだ不慣れなため、もしかしたら不適切な点や失礼な点があるかもしれません。もし何かお気づきの点や、ご不明な点がございましたら、お気軽にご連絡いただけると幸いです。
