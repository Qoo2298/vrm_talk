# VRM Talk 統合環境

キャラクター（VRM）とリアルタイムで会話するための、shisaku・COEIROINK・Ollama を連携させたフルスタック環境です。
COEIROINKのDocker環境は、以下をベースに構築していますが、安定動作のために改造を加えています。また、ライセンスが不明なため、これは個人の開発用スタックとして構成されており、再現環境の構築を目的としていません。
[kuwacom/COEIROINK-with-Docker](https://github.com/kuwacom/COEIROINK-with-Docker)

また、shisaku アプリケーションの構築にあたり、以下の記事を参考にしています。
[three-vrmを使ってブラウザ上にVRMを表示してみた](https://zenn.dev/yushimaten/articles/a853e3328db695)
## ✨ 機能紹介

- **デュアルAIエンジン**: 高性能なクラウドAI (Google Gemini) と、プライベートなローカルAI (Ollama) を、会話の目的に応じて切り替えられます。
- **感情連動型の音声合成**: AIが会話の文脈や感情を読み取り、COEIROINKの多彩な音声スタイル（喜び、しょんぼり、ごきげん等）を自動で選択します。表現力豊かな声でキャラクターが話します。
- **ダイナミックなアバター動作**: 会話内容に応じてキャラクターの首の動きなどを自動生成し、生き生きとしたインタラクションを実現します。
- **音声入力対応**: マイクに向かって話しかけることで、文字を打ずに会話を進められます。
- **Dockerによる簡単構築**: Docker Compose を使うことで、複雑な環境をコマンド一つで再現できます。
- **vrmファイル変更可**: お手持ちのvrmファイルをそのまま使うことができます。

> [!NOTE]
> 実際の動作の様子をデモ動画として公開しています。AIとの会話に応じてキャラクターの音声や表情、動作がリアルタイムで連携する様子をご覧いただけます。セキュリティの観点から、サーバーの一般公開は見合わせています。

> https://youtu.be/7Dp29A-9dHU
> https://youtu.be/tSdtG1rsPE4

> 動画で使用したモデル
> [フロウ](https://hub.vroid.com/characters/471902356051310196/models/1057268305479785163)
> [オルテンシア](https://hub.vroid.com/characters/2275990777783334731/models/5546300287879782053)

---

**注意:** 以下の「前提条件」から「トラブルシューティング」までの内容は、この環境が再現環境の構築を目的としていないため、記載通りに実行しても動作しません。あくまで参考情報としてご覧ください。

<del>

## 🛠️ 前提条件

この環境を動作させるには、お使いのPCに以下のソフトウェアがインストールされている必要があります。

- **Docker と Docker Compose**
- **NVIDIA製GPU** と、対応するグラフィックドライバ
  - > ローカルLLMを使用する場合、NVIDIA GeForce RTX 4060と同等かそれ以上を推奨します。
- **NVIDIA Container Toolkit**: DockerコンテナからGPUを利用するために必須です。
- **Git**: このリポジトリをクローンするために使用します。

> [!IMPORTANT]
> **(WSL2ユーザー向け)** WSL2のメモリ割り当てが少ないと、Ollamaの動作が極端に遅くなる場合があります。
> `%userprofile%` フォルダに `.wslconfig` ファイルを作成し、十分なメモリを割り当てることを推奨します。
> ```ini
> [wsl2]
> memory=16GB
> ```
> 設定後は `wsl --shutdown` コマンドでWSL2を再起動してください。

---

## 🚀 環境構築の手順

### ステップ1: リポジトリをクローン
```bash
git clone https://github.com/Qoo2298/vrm_talk.git
cd vrm_talk
```

### ステップ2: 環境変数の設定
`.env` という名前のファイルを、この `README.md` と同じ階層に作成し、中にご自身のGemini APIキーを記述してください。

```plaintext:C:/Users/shiny/AppData/Local/Programs/Microsoft VS Code/Ubuntu/home/owner/stack/.env
GEMINI_API_KEY=ここにあなたのAPIキーを貼り付け
```

### ステップ3: 不足しているアセットの配置 (重要！)
このリポジトリには、ライセンスや容量の都合上、モデルやエンジンそのものは含まれていません。以下の手順で、必要なアセットをダウンロードし、正しい場所に配置してください。

#### ① COEIROINK (エンジン & 音声モデル)

1.  **COEIROINK公式サイト**から、Linux GPU版の本体（.zip）をダウンロードします。
    - https://coeiroink.com/download
2.  ダウンロードしたzipファイルを展開し、`COEIROINK_LINUX_GPU` フォルダを、このリポジトリの `COEIROINK-with-Docker` フォルダの直下に配置します。
3.  **【重要】** まず、配置したフォルダ（`COEIROINK-with-Docker/COEIROINK_LINUX_GPU`）の中にあるGUIアプリケーション本体（`COEIROINK`など）を一度起動してください。
4.  アプリが起動したら、ソフトの画面上で使用したい音声モデルをすべてダウンロードします。
    - ダウンロードされたモデルは、自動的に、先ほど配置した `COEIROINK-with-Docker/COEIROINK_LINUX_GPU` フォルダ内の `speaker_info` フォルダに保存されていきます。
5.  必要なモデルをすべてダウンロードしたら、`COEIROINK_LINUX_GPU` フォルダ内にある `engine` フォルダと、モデルが保存された `speaker_info` フォルダの両方を、このリポジトリ（`vrm_talk`）の `COEIROINK-with-Docker` フォルダの直下にコピー（または移動）します。
    - （※つまり、`engine` と `speaker_info` が、`COEIROINK_LINUX_GPU` フォルダと同じ階層に来るようにします）

> [!TIP]
> 現状の `shisaku` アプリケーションでは、音声モデルとしてデフォルトで「MANA」が使用されます。
> 他の音声モデルを使用したい場合は、そのモデルの `speaker_id` と `style_id` を特定し、`shisaku` 側の設定ファイルを修正する必要があります。
> 詳細な設定方法やファイルの場所については、AIアシスタントに尋ねてみてください。

#### ② VRMモデル

1.  お好きなVRMモデルを用意します。ここでは例として、以下のモデルを使用します。
    - **VRoid Hub**: 「オルテンシア」
2.  ダウンロードしたVRMファイルを `avatar.vrm` という名前に変更します。
3.  名前を変更した `avatar.vrm` ファイルを、`shisaku/frontend/` フォルダの中に配置します。

> [!NOTE]
> VRMモデルを変更する場合、`shisaku`側の設定でデフォルト（例: `default.vrm`）となっている箇所を、配置したファイル名（例: `avatar.vrm`）に修正する必要がある場合があります。

#### ③ モーションデータ

1.  `shisaku` が使用するモーションデータを、以下のBOOTHページからダウンロードします。
    - **BOOTH**: VRMおしゃべりモーション
2.  `shisaku/frontend/` フォルダの中に、新しく `vrma` という名前のフォルダを作成してください。（このフォルダはデフォルトでは存在しません）
3.  ダウンロードしたフォルダの中身を、すべて新しく作成した `vrma` フォルダの中に配置します。

### 最終的なプロジェクトのディレクトリ構成
すべてのファイルとアセットを配置すると、プロジェクトのトップレベルは以下のようになります。

```plaintext
vrm_talk/
├── .env                    # 環境変数ファイル
├── .gitignore
├── COEIROINK-with-Docker/  # COEIROINKエンジンとモデル
│   ├── engine/             # 👈 配置したエンジン
│   ├── speaker_info/       # 👈 配置した音声モデル
│   └── ...
├── ollama/                 # Ollamaのデータボリューム
├── shisaku/                # shisakuアプリケーション本体
│   ├── backend/
│   └── frontend/
│       ├── avatar.vrm      # 👈 配置したVRMモデル
│       └── vrma/           # 👈 配置したモーションデータ
│           └── ...
├── docker-compose.yml
└── README.md
```

### ステップ4: コンテナのビルドと起動
すべてのアセットを配置したら、以下のコマンドで3つのコンテナをまとめてビルDし、バックグラウンドで起動します。

```bash
docker compose up -d --build
```

### ステップ5: Ollama AIモデルのダウンロード
コンテナは起動しましたが、まだOllamaの中にAIモデルが入っていません。
以下のコマンドを実行して、shisaku が使用する2つのモデルをダウンロードしてください。（完了まで少し時間がかかります）

```bash
# 軽量モデル
docker compose exec ollama ollama pull gemma:2b

# 高性能モデル (任意)
docker compose exec ollama ollama pull llama3
```

---

## 🖥️ アプリケーションの使用方法
全てのセットアップが完了したら、以下のURLにブラウザでアクセスしてください。

- **メイン画面**: `http://localhost:8000`

各APIサーバーの動作確認は、以下のURLから行えます。
- **COEIROINK API**: `http://localhost:50032`
- **Ollama API**: `http://localhost:11434`

### ネットワークアクセスについて (重要！)
デフォルトの docker-compose.yml の設定 (ports: - "8000:8000") では、アプリケーションはそのPC自身 (localhost) からのアクセスのみを受け付けます。

<details>
<summary><strong>同じLAN内の他のデバイス (スマホ、別のPCなど) からアクセスしたい場合</strong></summary>

`docker-compose.yml` 内の各サービスの `ports` 設定を、以下のように変更してください。

```yaml
# 例: shisakuサービスの場合
services:
  shisaku:
    ports:
      - "0.0.0.0:8000:8000"
```

`0.0.0.0` を追加することで、そのPCのすべてのネットワークインターフェースからのアクセスを受け付けるようになります。変更後は `docker compose up -d --force-recreate` でコンテナを再作成してください。

</details>

<details>
<summary><strong>インターネット (グローバル) からアクセスしたい場合</strong></summary>

`0.0.0.0` の設定に加えて、以下の設定が必要です。

- **ルーターのポートフォワーディング**: インターネットの出入り口にあるルーターで、外部からのアクセスをサーバーの該当ポートに転送する設定が必要です。
- **サーバーのファイアウォール設定**: サーバー自身のファイアウォールで、該当ポートへのアクセスを許可する設定が必要です。

> [!WARNING]
> これらの設定を行わない限り、`0.0.0.0` に設定してもインターネットから直接公開されることはありません。セキュリティ設定は慎重に行ってください。

</details>

---

## 🤔 トラブルシューティング

- **コンテナが起動しない**:
  `docker compose up` を `-d` なしで実行すると、詳細なログを確認できます。ポートの競合（`address already in use`）が起きている場合は、他のアプリが同じポートを使っていないか確認するか、`docker-compose.yml` の `ports` の設定を変更してください。

- **Ollamaの応答が異常に遅い**:
  WSL2のメモリ不足が考えられます。「前提条件」の `.wslconfig` の設定を確認してください。

</del>

---

## 📝 開発後記
この環境をつくるまでには、思った以上に手こずることも多く、そのぶん得られた気づきや学びもたくさんありました。特に印象に残っている点を、いくつか挙げておきます。

### 苦労した点
- **COEIROINKとの格闘**: Linux版は用意されているものの、基本はGUI操作が前提。ターミナルだけで安定して動かすのにはかなり苦労しました。調べても情報が少なく、何度も試してはエラーにぶつかり、原因が分からないまま進めない日も多かったです。
- **DockerとGitとの出会い**: 開発当初はDockerやGitの知識が全くなく、これらのツールを理解し、連携させることに多くの時間を費やしました。特にGitHubへの初めてのプッシュは、コミットの概念やアクセストークンでの認証など、慣れない作業の連続で悪戦苦闘しました。
- **WindowsからLinuxへの移行**: 最初はWindows環境で動作させていましたが、サーバー運用を視野に入れLinux環境への移行を決断しました。しかし、WindowsのGUIで動いていたCOEIROINKが、LinuxのCLI環境ではそのままでは動作しないという壁に直面し、この点が最も困難でした。

### 開発のパートナー、生成AI
- **Geminiとともに**: 開発のほとんどの段階で、GeminiあるいはCodexの助けを借りました。特に、原因不明のエラーを調べるときや、コードを一から書くときは頼りになりました。Geminiがコードを書いてくれたおかげで、わたし自身は動作確認や検証に集中できました。

このプロジェクトを通じて、Dockerによる環境構築の重要性、Gitによるバージョン管理の便利さ、そしてAIの可能性を深く実感しました。長い道のりでしたが、最終的にこの環境が動作したときの感動は忘れられません。このリポジトリが、同じような課題に直面する方々の一助となれば幸いです。

### 今後の課題と考察
表情や動きの処理をLLMにまかせると、どうしても遅延が出てしまいます。12Bクラスの大きなモデルも試しましたが、ローカル環境では動作が重く、結果として軽量なモデルが現実的だと感じました。
クラウドのGemini APIでも多少のラグは避けられないので、今後は非同期処理やバックグラウンド実行の工夫で、もう少しスムーズにできないか探っていくつもりです。

ただ、AIだけに頼りすぎるのも危ういと感じました。知識がないまま使おうとすると、思わぬところで詰まってしまうし、現場ではそもそもAIが使えないこともあります。ある程度は自分の手でコードが書けるようにならないとダメだなと、あらためて思いました。
特にUI周りはAI任せだとズレが起きやすく、最終的には自分で微調整が必要です。何度もエラーが出たものの、原因がわからないまま直ったことも多く、「現場ってこういうことか」と少し実感できた気がします。

### 開発の動機と未来への展望
そもそも、こういった統合環境をつくるのは、ずっとやってみたかったことでした。最初はコードなんて書けなかったし、何をどう始めていいかも分からなかったけど、gemini cliのことを知り、「自分でもやれるかも」と思って手を動かし始めました。

もともとはLLMと動作データ、視聴覚データ、表情を融合させた統合モデルを再現したいと考えていましたが、現状ではどうしても統合が難しく、ぎこちない動作になってしまいます。これは、そのうちビッグテックが実現してくれると信じています。このプロジェクトは、その未来に向けた自分なりの第一歩です。

X（旧Twitter）では、Grokを活用してアニメキャラのように振る舞う例も出てきていますが、わたしの目指すのも、その延長にある未来です。
キャラクターがただ喋るだけじゃなくて、感情や文脈に合わせて動きや表情を変えてくれる、そんな自然なやりとりができる世界を目指しています。

最近はヒューマノイドロボットにも強く関心があり、人とAIがもっと身近に、自然に共存できる未来づくりに関わっていきたいと考えています。

### GitHub初心者としてひとこと
GitHubの利用にはまだ不慣れなため、もしかしたら不適切な点や失礼な点があるかもしれません。もし何かお気づきの点や、ご不明な点がございましたら、お気軽にご連絡いただけると幸いです。
