VRM Talk 統合環境

キャラクター（VRM）とリアルタイムで会話するための、shisaku・COEIROINK・Ollama を連携させたフルスタック環境です。
COEIROINKのDocker環境は、kuwacom/COEIROINK-with-Docker をベースに構築していますが、安定動作のために結構改造しています。また、ライセンスが不明なため、これは個人の開発用スタックとして構成されており、再現環境の構築を目的としていません。
また、shisaku アプリケーションの構築にあたり、three-vrmを使ってブラウザ上にVRMを表示してみたを参考にしています。

✨ 機能紹介

デュアルAIエンジン: 高性能なクラウドAI (Google Gemini) と、プライベートなローカルAI (Ollama) を、会話の目的に応じて切り替えられます。

感情連動型の音声合成: AIが会話の文脈や感情を読み取り、COEIROINKの多彩な音声スタイル（喜び、しょんぼり、ごきげん等）を自動で選択します。表現力豊かな声でキャラクターが話します。

ダイナミックなアバター動作: 会話内容に応じてキャラクターの首の動きなどを自動生成し、生き生きとしたインタラクションを実現します。

音声入力対応: マイクに向かって話しかけることで、文字を打ずに会話を進められます。

Dockerによる簡単構築: Docker Compose を使うことで、複雑な環境をコマンド一つで再現できます。

（※実際の動作の様子をデモ動画として公開しています。AIとの会話に応じてキャラクターの音声や表情、動作がリアルタイムで連携する様子をご覧いただけます。セキュリティの観点から、サーバーの一般公開は見合わせています。）

~~

1. 前提条件

この環境を動作させるには、お使いのPCに以下のソフトウェアがインストールされている必要があります。

Docker と Docker Compose

NVIDIA製GPU と、対応するグラフィックドライバ（ローカルLLMを使用する場合、NVIDIA GeForce RTX 4060と同等かそれ以上を推奨します）

NVIDIA Container Toolkit: DockerコンテナからGPUを利用するために必須です。

インストールガイド

Git: このリポジトリをクローンするために使用します。

(WSL2ユーザー向け) WSL2のメモリ割り当てが少ないと、Ollamaの動作が極端に遅くなる場合があります。%userprofile% フォルダに .wslconfig ファイルを作成し、十分なメモリを割り当てることを推奨します。

[wsl2]
memory=16GB









設定後は wsl --shutdown コマンドでWSL2を再起動してください。

2. 環境構築の手順

ステップ1: リポジトリをクローン

git clone [https://github.com/Qoo2298/vrm_talk.git](https://github.com/Qoo2298/vrm_talk.git)
cd vrm_talk


ステップ2: 環境変数の設定

.env という名前のファイルを、この README.md と同じ階層に作成し、中にご自身のGemini APIキーを記述してください。

GEMINI_API_KEY=ここにあなたのAPIキーを貼り付け


ステップ3: 不足しているアセットの配置 (重要！)

このリポジトリには、ライセンスや容量の都合上、モデルやエンジンそのものは含まれていません。以下の手順で、必要なアセットをダウンロードし、正しい場所に配置してください。

① COEIROINK (エンジン & 音声モデル)

COEIROINK公式サイトから、Linux GPU版の本体（.zip）をダウンロードします。

https://coeiroink.com/download

ダウンロードしたzipファイルを展開し、COEIROINK_LINUX_GPU フォルダを、このリポジトリの COEIROINK-with-Docker フォルダの直下に配置します。

【重要】まず、配置したフォルダ（COEIROINK-with-Docker/COEIROINK_LINUX_GPU）の中にあるGUIアプリケーション本体（COEIROINK_LINUX_GPUなど）を一度起動してください。

アプリが起動したら、ソフトの画面上で使用したい音声モデルをすべてダウンロードします。
ダウンロードされたモデルは、自動的に、先ほど配置した COEIROINK-with-Docker/COEIROINK_LINUX_GPU フォルダ内の speaker_info フォルダに保存されていきます。

必要なモデルをすべてダウンロードしたら、COEIROINK_LINUX_GPU フォルダ内にある engine フォルダと、モデルが保存された speaker_info フォルダの両方を、このリポジトリ（vrm_talk）の COEIROINK-with-Docker フォルダの直下にコピー（または移動）します。
（※つまり、engine と speaker_info が、COEIROINK_LINUX_GPU フォルダと同じ階層に来るようにします）

vrm_talk/
├── COEIROINK-with-Docker/
│   ├── COEIROINK_LINUX_GPU/  
│   ├── engine/               
│   ├── speaker_info/         
│   ├── Dockerfile
│   └── ... (その他のCOEIROINK関連ファイル)
└── ...





現状の shisaku アプリケーションでは、音声モデルとしてデフォルトで「MANA」が使用される設定になっています。
もし他の音声モデル（ご自身でダウンロードしたモデルなど）を使用したい場合は、そのモデルの speaker_id と style_id を実際に調べて特定し、shisaku 側の設定ファイルに記述する必要があります。
詳細な設定方法やファイルの場所については、CodexまたはGeminiに尋ねてみてください。

② VRMモデル

お好きなVRMモデルを用意します。ここでは例として、以下のモデルを使用します。

VRoid Hub: 「オルテンシア」

ダウンロードしたVRMファイルを avatar.vrm という名前に変更します。

名前を変更した avatar.vrm ファイルを、shisaku/frontend/ フォルダの中に配置します。

（※VRMモデルを変更する場合、shisaku側の設定でデフォルト（例: default.vrm）となっている箇所を、配置したファイル名（例: avatar.vrm）に修正する必要がある場合があります。）

③ モーションデータ

shisaku が使用するモーションデータを、以下のBOOTHページからダウンロードします。

BOOTH: VRMおしゃべりモーション

shisaku/frontend/ フォルダの中に、新しく vrma という名前のフォルダを作成してください。（このフォルダはデフォルトでは存在しません）

ダウンロードしたフォルダの中身を、すべて新しく作成した vrma フォルダの中に配置します。

最終的なプロジェクトのディレクトリ構成

すべてのファイルとアセットを配置すると、プロジェクトのトップレベルは以下のようになります。

vrm_talk/
├── COEIROINK-with-Docker/  # COEIROINKエンジンとモデル
│   ├── engine/           # 配置したエンジン
│   ├── speaker_info/     # 配置した音声モデル
│   └── ...
├── ollama/                 # Ollamaのデータボリューム
├── shisaku/                # shisakuアプリケーション本体
│   ├── backend/
│   ├── frontend/
│   │   ├── avatar.vrm      # 配置したVRMモデル
│   │   └── vrma/           # 配置したモーションデータ
│   └── ...
├── .env                    # 環境変数ファイル
├── .gitignore              # Gitの無視設定
├── docker-compose.yml      # Docker Compose設定ファイル
└── README.md               # この説明ファイル


ステップ4: コンテナのビルドと起動

すべてのアセットを配置したら、以下のコマンドで3つのコンテナをまとめてビルドし、バックグラウンドで起動します。

docker compose up -d --build


ステップ5: Ollama AIモデルのダウンロード

コンテナは起動しましたが、まだOllamaの中にAIモデルが入っていません。
以下のコマンドを実行して、shisaku が使用する2つのモデルをダウンロードしてください。（完了まで少し時間がかかります）

# 軽量モデル (gemma3:4b)
docker compose exec ollama ollama pull gemma3:4b

# 高性能モデル (gemma3:12b)
docker compose exec ollama ollama pull gemma3:12b


3. アプリケーションの使用方法

全てのセットアップが完了したら、以下のURLにブラウザでアクセスしてください。

メイン画面: http://localhost:8000

各APIサーバーの動作確認は、以下のURLから行えます。

COEIROINK API: http://localhost:50032

Ollama API: http://localhost:11434

ネットワークアクセスについて (重要！)

デフォルトの docker-compose.yml の設定 (ports: - "8000:8000") では、アプリケーションはそのPC自身 (localhost) からのアクセスのみを受け付けます。

同じLAN内の他のデバイス (スマホ、別のPCなど) からアクセスしたい場合:
docker-compose.yml 内の各サービスの ports 設定を、以下のように変更してください。

ports:
  - "0.0.0.0:8000:8000" # 例: shisakuサービスの場合









0.0.0.0 を追加することで、そのPCのすべてのネットワークインターフェースからのアクセスを受け付けるようになります。変更後は docker compose up -d --force-recreate でコンテナを再作成してください。

インターネット (グローバル) からアクセスしたい場合:
0.0.0.0 の設定に加えて、以下の設定が必要です。

ルーターのポートフォワーディング: インターネットの出入り口にあるルーターで、外部からのアクセスをサーバーの該当ポートに転送する設定が必要です。

サーバーのファイアウォール設定: サーバー自身のファイアウォールで、該当ポートへのアクセスを許可する設定が必要です。

これらの設定を行わない限り、0.0.0.0 に設定してもインターネットから直接公開されることはありません。

4. トラブルシューティング

コンテナが起動しない: docker compose up を -d なしで実行すると、詳細なログを確認できます。ポートの競合（address already in use）が起きている場合は、他のアプリが同じポートを使っていないか確認するか、docker-compose.yml の ports の設定を変更してください。

Ollamaの応答が異常に遅い: WSL2のメモリ不足が考えられます。「前提条件」の .wslconfig の設定を確認してください。
~~

5. 開発を終えて / 感想

この環境の構築は、私にとって非常に挑戦的で、多くの学びがありました。特に印象的だったのは、以下の点です。

COEIROINKとの格闘: Linux版は提供されているものの、GUIを前提とした設計のため、CLI環境で安定動作させるまでに大変な苦労がありました。参照サイトを読み込み、何度も試行錯誤を繰り返しましたが、エラーの原因が分からず、途方に暮れることも少なくありませんでした。

DockerとGitとの出会い: 開発当初はDockerやGitの知識が全くなく、これらのツールを理解し、連携させることに多くの時間を費やしました。特にGitHubへの初めてのプッシュは、コミットの概念やアクセストークンでの認証など、慣れない作業の連続で悪戦苦闘しました。

WindowsからLinuxへの移行: 最初はWindows環境で動作させていましたが、サーバー運用を視野に入れLinux環境への移行を決断しました。しかし、WindowsのGUIで動いていたCOEIROINKが、LinuxのCLI環境ではそのままでは動作しないという壁に直面し、この点が最も困難な課題の一つでした。

Gemini CLI (GPT-5 Codex) との協業: 開発のほとんどの段階で、Gemini CLI (GPT-5 Codex) の助けを借りました。特に、エラーの原因究明やコードの記述において、そのサポートは不可欠でした。多くのコードはGemini CLIに書いてもらい、私自身は試行錯誤と検証に集中することができました。

このプロジェクトを通じて、Dockerによる環境構築の重要性、Gitによるバージョン管理の便利さ、そしてAIとの協業の可能性を深く実感しました。長い道のりでしたが、最終的にこの環境が動作したときの感動は忘れられません。このリポジトリが、同じような課題に直面する方々の一助となれば幸いです。

表情や動作をLLMに委ねると、どうしてもラグが発生してしまうため、両立が大事だと感じました。
12Bモデルも試しましたが、ローカルモデルでは結局のところ遅延を考慮すると4Bモデルが妥当という結論に至りました。
現状ではクラウドのGemini APIを使用した場合でも若干の遅延は発生してしまうため、今後は裏処理を工夫するなどして、この遅延をさらに短縮できるようにしたいと考えています。

ただ、知識がない状態で生成AIに委ねることは大変でした。頼りすぎるのもよくないと感じます。
頼りすぎた点は課題として残りました。
やはり現場ではセキュリティ等の問題で生成AIが使えないこともあるため、自身である程度はコードが書けるようになる必要性を実感しました。
生成AIだと、どうしてもWeb UIがズレてしまうことがあります。いずれは完璧になると思われますが、現状では調整が必要です。
エラーがなぜ発生し、なぜ解決したかがわからない場面も多々ありました。
実際に実装は大変苦戦し、現場の方々の気持ちが少しわかったような気がします。

開発の動機と未来への展望

そもそも、このような統合環境を開発することには以前から憧れがありました。コードを書く技術は持ち合わせていませんでしたが、IT業界を志望する中で、最近では生成AIがコード作成や相談相手になってくれることを知り、このプロジェクトに着手しました。

もともとはLLMと動作データ、視聴覚データ、表情を融合させた統合モデルを再現したいと考えていましたが、現状ではどうしても統合が難しく、ぎこちない動作になってしまいます。これは、そのうちビッグテックが実現してくれると信じています。このプロジェクトは、その壮大な未来への私Nの第一歩です。

X（旧Twitter）では、Grokを活用したAniのような事例がありますが、このプロジェクトはその延長線上にある未来を意識しています。キャラクターがただ話すだけでなく、感情や文脈に合わせて自然に動き、より豊かなコミュニケーションが生まれる世界を目指しています。

現在、イーロン・マスク氏率いるオプティマスのようなヒューマノイドロボットにも強い関心があり、人間とAIがより密接に共生する未来の実現に貢献したいと考えています。

【GitHub初心者からのメッセージ】

GitHubの利用にはまだ不慣れなため、もしかしたら不適切な点や失礼な点があるかもしれません。もし何かお気づきの点や、ご不明な点がございましたら、お気軽にご連絡いただけると幸いです。
